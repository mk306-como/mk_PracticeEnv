{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `QuantumPioneer/databases` Demo\n",
    "This notebook demonstrates how to interact with the `QuantumPioneer` databases. It does:\n",
    "1) reads the QP DLPNO demo database from the local directory.\n",
    "2) creates a full and short CSV version \n",
    "3) creates an OWL ABOX file based on an OWL file created from the DLPNO schema\n",
    "4) it uploads the OWL ABOX file to a local tripple store - blazegraph\n",
    "5) performs various SPARQL queries on the small data base. \n",
    "\n",
    "See inline comments for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# both pandas and polars can read parquet files - pick whichever you prefer!\n",
    "# there are advantages and disadvantages to both\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import pyarrow as pa\n",
    "# this library interacts with the parquet format directly, and both pandas and polars can use it too\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# schema = layout of the database (what are the datatypes, etc.)\n",
    "# the schema for the quantumpioneer databases are stored in databases.schema and vary depending on the type of data\n",
    "from databases.schema import DLPNO_SCHEMA, DFT_SCHEMA\n",
    "\n",
    "# import libraries for KG handling\n",
    "from owlready2 import *\n",
    "import csv\n",
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, XSD, OWL\n",
    "import os\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLPNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to match wherever you have the database file located\n",
    "# DLPNO_DATABASE_FPATH = \"QuantumPioneer_v1_DLPNO.parquet\"  original code Jackson\n",
    "# DLPNO_DATABASE_FPATH = \"C:\\\\VS mk306 TestEnv\\\\QuantumPioneer\\\\databases\\\\dlpno_ts_v5.parquet\"    mk306 Jackson folder \n",
    "DLPNO_DATABASE_FPATH = \"C:\\\\VS mk306 TestEnv\\\\Codes\\\\mk_PracticeEnv\\\\dlpno_ts_v5.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              source  \\\n",
      "0  /data1/groups/RMG/Projects/Hao-Wei-Oscar-Yunsi...   \n",
      "1  /data1/groups/RMG/Projects/Hao-Wei-Oscar-Yunsi...   \n",
      "2  /data1/groups/RMG/Projects/Hao-Wei-Oscar-Yunsi...   \n",
      "3  /data1/groups/RMG/Projects/Hao-Wei-Oscar-Yunsi...   \n",
      "\n",
      "                                       route_section  charge  multiplicity  \\\n",
      "0  uHF UNO DLPNO-CCSD(T)-F12D cc-pvtz-f12 def2/J ...       0             2   \n",
      "1  uHF UNO DLPNO-CCSD(T)-F12D cc-pvtz-f12 def2/J ...       0             2   \n",
      "2  uHF UNO DLPNO-CCSD(T)-F12D cc-pvtz-f12 def2/J ...       0             2   \n",
      "3  uHF UNO DLPNO-CCSD(T)-F12D cc-pvtz-f12 def2/J ...       0             2   \n",
      "\n",
      "       energy  run_time                                  input_coordinates  \\\n",
      "0 -552.285416    2401.0  [[-1.18079, 1.675527, 1.578389], [-0.208623, 2...   \n",
      "1 -570.750611    2936.0  [[2.188448, 0.23618, -1.582947], [1.462597, -0...   \n",
      "2 -572.077854    2755.0  [[1.863694, -1.837741, 0.404006], [1.711148, -...   \n",
      "3 -572.156665    2168.0  [[0.511578, 0.143144, 1.144126], [0.099432, -0...   \n",
      "\n",
      "   dipole_au  t1_diagnostic  \n",
      "0    1.07331       0.016131  \n",
      "1    1.87994       0.021023  \n",
      "2    1.17816       0.021460  \n",
      "3    2.02224       0.015178  \n",
      "Data successfully exported to myQPCSVhead10.csv\n"
     ]
    }
   ],
   "source": [
    "# just open and read the entire dataset (very practical with the DLPNO data, which is small), which will be slow with pandas\n",
    "df = pd.read_parquet(\n",
    "    DLPNO_DATABASE_FPATH,\n",
    "    schema=DLPNO_SCHEMA,  # pandas will try and guess this on its own if you don't provide it - it gets it right, but is slower\n",
    ")\n",
    "df.head(4)\n",
    "\n",
    "# Optionally view the first 4 rows\n",
    "print(df.head(4))\n",
    "\n",
    "# Export the DataFrame to a CSV file without the index\n",
    "output_csv = 'myQPCSVhead10.csv'\n",
    "df.head(10).to_csv(output_csv, index=False)\n",
    "# df.to_csv(output_csv, index=False)\n",
    "print(f\"Data successfully exported to {output_csv}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ontology from: ontoQP-DPLO.owl\n",
      "Processing CSV file: myQPCSVhead10.csv\n",
      "Processing row 1\n",
      "Processing row 2\n",
      "Processing row 3\n",
      "Processing row 4\n",
      "Processing row 5\n",
      "Processing row 6\n",
      "Processing row 7\n",
      "Processing row 8\n",
      "Processing row 9\n",
      "Processing row 10\n",
      "Saving output to: ontoQP-DPLO_abox.owl\n",
      "Successfully created OWL file with ABox at: ontoQP-DPLO_abox.owl\n"
     ]
    }
   ],
   "source": [
    "# write CSV to OWL\n",
    "\n",
    "def csv_to_owl(csv_file, owl_tbox_file, output_owl_file):\n",
    "    try:\n",
    "        # Verify files exist\n",
    "        if not os.path.exists(csv_file):\n",
    "            raise FileNotFoundError(f\"CSV file not found: {csv_file}\")\n",
    "        if not os.path.exists(owl_tbox_file):\n",
    "            raise FileNotFoundError(f\"OWL TBox file not found: {owl_tbox_file}\")\n",
    "\n",
    "        # Initialize RDF Graph\n",
    "        g = Graph()\n",
    "        \n",
    "        # Parse the existing TBox ontology\n",
    "        print(f\"Loading ontology from: {owl_tbox_file}\")\n",
    "        g.parse(owl_tbox_file, format='xml')\n",
    "        \n",
    "        # Define namespaces - using the same URI as in your ontology file\n",
    "        base_ns = Namespace(\"http://example.org/ontologies/DLPNO.owl#\")\n",
    "        g.bind(\"dlpno\", base_ns)\n",
    "        \n",
    "        # Read CSV file and create individuals\n",
    "        print(f\"Processing CSV file: {csv_file}\")\n",
    "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            \n",
    "            for i, row in enumerate(reader, 1):\n",
    "                try:\n",
    "                    print(f\"Processing row {i}\")\n",
    "                    # Create a unique URI for each individual\n",
    "                    individual_uri = URIRef(base_ns[f\"result_{i}\"])\n",
    "                    \n",
    "                    # Add type assertion\n",
    "                    g.add((individual_uri, RDF.type, base_ns.DLPNOResult))\n",
    "                    \n",
    "                    # Add properties from CSV with proper error handling\n",
    "                    properties = [\n",
    "                        ('source', 'hasSource', XSD.string, str),\n",
    "                        ('route_section', 'hasRouteSection', XSD.string, str),\n",
    "                        ('charge', 'hasCharge', XSD.unsignedByte, int),\n",
    "                        ('multiplicity', 'hasMultiplicity', XSD.unsignedByte, int),\n",
    "                        ('energy', 'hasEnergy', XSD.double, float),\n",
    "                        ('run_time', 'hasRunTime', XSD.float, float),\n",
    "                        ('dipole_au', 'hasDipoleAU', XSD.float, float),\n",
    "                        ('t1_diagnostic', 'hasT1Diagnostic', XSD.float, float)\n",
    "                    ]\n",
    "                    \n",
    "                    for csv_col, prop_name, xsd_type, type_func in properties:\n",
    "                        if csv_col in row:\n",
    "                            try:\n",
    "                                value = type_func(row[csv_col]) if row[csv_col] else None\n",
    "                                if value is not None:\n",
    "                                    prop_uri = URIRef(base_ns + prop_name)\n",
    "                                    g.add((individual_uri, prop_uri, Literal(value, datatype=xsd_type)))\n",
    "                            except (ValueError, TypeError) as e:\n",
    "                                print(f\"Warning: Could not process {csv_col} value '{row[csv_col]}': {str(e)}\")\n",
    "                    \n",
    "                    # Special handling for coordinates\n",
    "                    if 'input_coordinates' in row:\n",
    "                        coords = row['input_coordinates'].strip('\"\\'')\n",
    "                        g.add((individual_uri, base_ns.hasInputCoordinates, \n",
    "                              Literal(coords, datatype=XSD.string)))\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row {i}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        # Serialize the complete ontology\n",
    "        print(f\"Saving output to: {output_owl_file}\")\n",
    "        g.serialize(destination=output_owl_file, format='xml')\n",
    "        print(f\"Successfully created OWL file with ABox at: {output_owl_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Usage with your actual filenames\n",
    "csv_to_owl(\n",
    "    csv_file='myQPCSVhead10.csv',\n",
    "    owl_tbox_file='ontoQP-DPLO.owl',\n",
    "    output_owl_file='ontoQP-DPLO_abox.owl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# upload the OWL file to local tripple store (Blazegraph) \n",
    "# # Blazegraph endpoint - can only read\n",
    "# blazegraph_url = \"http://localhost:3838/blazegraph/namespace/kb/sparql\"\n",
    "\n",
    "# Blazegraph endpoint can also write\n",
    "blazegraph_url = \"http://localhost:3838/blazegraph/namespace/kb/update\"\n",
    "\n",
    "\n",
    "# Path to your RDF file\n",
    "rdf_file_path = r\"C:\\VS mk306 TestEnv\\Codes\\mk_PracticeEnv\\ontoQP-DPLO_abox.owl\"\n",
    "\n",
    "\n",
    "# Headers for the request\n",
    "# headers = {\n",
    "#     \"Content-Type\": \"text/turtle\"\n",
    "# }\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/rdf+xml\"\n",
    "}\n",
    "\n",
    "# Read the RDF file\n",
    "with open(rdf_file_path, \"r\") as file:\n",
    "    rdf_data = file.read()\n",
    "\n",
    "# Send the data to Blazegraph\n",
    "response = requests.post(blazegraph_url, headers=headers, data=rdf_data)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Data loaded successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to load data. Status code: {response.status_code}\")\n",
    "    print(response.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.org/ontologies/DLPNO.owl#result_5 http://example.org/ontologies/DLPNO.owl#hasCharge 0\n",
      "http://example.org/ontologies/DLPNO.owl#result_5 http://example.org/ontologies/DLPNO.owl#hasDipoleAU 0.85609\n",
      "Result: http://example.org/ontologies/DLPNO.owl#result_9 | Energy: -611.72942540614\n",
      "Result: http://example.org/ontologies/DLPNO.owl#result_6 | Energy: -589.360925465229\n",
      "Result: http://example.org/ontologies/DLPNO.owl#result_8 | Energy: -575.799460863625\n",
      "Result: http://example.org/ontologies/DLPNO.owl#result_10 | Runtime: 3488.0\n",
      "Result: http://example.org/ontologies/DLPNO.owl#result_2 | Runtime: 2936.0\n",
      "Result: http://example.org/ontologies/DLPNO.owl#result_8 | Runtime: 2803.0\n",
      "Result: http://example.org/ontologies/DLPNO.owl#result_1 | Multiplicity: 2 | Charge: 0\n",
      "Result: http://example.org/ontologies/DLPNO.owl#result_10 | Multiplicity: 2 | Charge: 0\n",
      "Result: http://example.org/ontologies/DLPNO.owl#result_2 | Multiplicity: 2 | Charge: 0\n",
      "Result: http://example.org/ontologies/DLPNO.owl#result_3 | T1 Diagnostic: 0.021459663\n",
      "Result: http://example.org/ontologies/DLPNO.owl#result_2 | T1 Diagnostic: 0.021022726\n",
      "Result: http://example.org/ontologies/DLPNO.owl#result_1 | Source: /data1/groups/RMG/Projects/Hao-Wei-Oscar-Yunsie/HAbs_calculations/ts_calculations/calculations/sep1a/output/DLPNO_sp_f12/outputs/outputs_48/id48644.log | Coordinates: [array([-1.18079 ,  1.675527,  1.578389])\n",
      " array([-0.208623,  2.492819,  1.043383])\n",
      " array([-0.779649,  1.343507,  2.394862])\n",
      " array([ 0.39244 ,  2.157912, -2.692596])\n",
      " array([-0.621929,  0.546014, -1.284805])\n",
      " array([-0.971654,  1.756936, -2.170265])\n",
      " array([ 0.817017,  2.041762, -4.123272])\n",
      " array([ 0.68777 ,  3.397201, -3.461238])\n",
      " array([ 0.738187,  1.006404, -0.756896])\n",
      " array([ 1.345064,  1.751076, -1.725896])\n",
      " array([-0.523152, -0.705763, -2.043505])\n",
      " array([-1.337158,  0.408528, -0.460266])\n",
      " array([-1.38121 ,  2.545217, -1.521893])\n",
      " array([-1.689654,  1.529796, -2.967584])\n",
      " array([ 1.825474,  1.662826, -4.301803])\n",
      " array([ 0.068298,  1.737138, -4.858615])\n",
      " array([ 1.610191,  3.922564, -3.20463 ])\n",
      " array([-0.151878,  4.041207, -3.734844])\n",
      " array([0.386656, 1.786164, 0.227104])\n",
      " array([ 1.418418,  0.261918, -0.320936])\n",
      " array([-0.436956, -1.690886, -2.642081])]\n"
     ]
    }
   ],
   "source": [
    "# Perform SPRQL query on local blazegraph \n",
    "# Blazegraph SPARQL Query Endpoint\n",
    "sparql_endpoint = blazegraph_url\n",
    "\n",
    "#--- SPARQL Query  - find all triples\n",
    "sparql_query = \"\"\"\n",
    "SELECT ?s ?p ?o\n",
    "WHERE {\n",
    "    ?s ?p ?o .\n",
    "} LIMIT 2\n",
    "\"\"\"\n",
    "\n",
    "# Send HTTP request\n",
    "response = requests.get(sparql_endpoint, params={\"query\": sparql_query, \"format\": \"json\"})\n",
    "\n",
    "# Parse JSON response\n",
    "results = response.json()\n",
    "\n",
    "# Print results\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"s\"][\"value\"], result[\"p\"][\"value\"], result[\"o\"][\"value\"])\n",
    "\n",
    "\n",
    "# --- Query 1: Retrieve all DLPNO results and their total energies.\n",
    "sparql_query = \"\"\"\n",
    "PREFIX dl: <http://example.org/ontologies/DLPNO.owl#>\n",
    "\n",
    "SELECT ?result ?energy\n",
    "WHERE {\n",
    "  ?result a dl:DLPNOResult ;\n",
    "          dl:hasEnergy ?energy .\n",
    "}\n",
    "ORDER BY ?energy\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "# Send HTTP request\n",
    "response = requests.get(\n",
    "    sparql_endpoint,\n",
    "    params={\"query\": sparql_query, \"format\": \"json\"}\n",
    ")\n",
    "response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "# Parse JSON response\n",
    "results = response.json()\n",
    "\n",
    "# Print results (use correct variable names: ?result, ?energy)\n",
    "for binding in results[\"results\"][\"bindings\"]:\n",
    "    print(\n",
    "        \"Result:\", binding[\"result\"][\"value\"],\n",
    "        \"| Energy:\", binding[\"energy\"][\"value\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Query 2: Find DLPNO calculations with runtime longer than 5000 seconds.\n",
    "sparql_query = \"\"\"\n",
    "PREFIX dl: <http://example.org/ontologies/DLPNO.owl#>\n",
    "\n",
    "SELECT ?result ?runtime\n",
    "WHERE {\n",
    "  ?result a dl:DLPNOResult ;\n",
    "          dl:hasRunTime ?runtime .\n",
    "  FILTER(?runtime > 2500)\n",
    "}\n",
    "ORDER BY DESC(?runtime)\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "# Send HTTP request\n",
    "response = requests.get(\n",
    "    sparql_endpoint,\n",
    "    params={\"query\": sparql_query, \"format\": \"json\"}\n",
    ")\n",
    "response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "# Parse JSON response\n",
    "results = response.json()\n",
    "\n",
    "# Print results (corrected variable names: ?result, ?runtime)\n",
    "for binding in results[\"results\"][\"bindings\"]:\n",
    "    print(\n",
    "        \"Result:\", binding[\"result\"][\"value\"],\n",
    "        \"| Runtime:\", binding[\"runtime\"][\"value\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Query 3: Extract DLPNO results by multiplicity and charge.\n",
    "sparql_query = \"\"\"\n",
    "PREFIX dl: <http://example.org/ontologies/DLPNO.owl#>\n",
    "\n",
    "SELECT ?result ?multiplicity ?charge\n",
    "WHERE {\n",
    "  ?result a dl:DLPNOResult ;\n",
    "          dl:hasMultiplicity ?multiplicity ;\n",
    "          dl:hasCharge ?charge .\n",
    "}\n",
    "ORDER BY ?charge ?multiplicity\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "# Send HTTP request\n",
    "response = requests.get(\n",
    "    sparql_endpoint,\n",
    "    params={\"query\": sparql_query, \"format\": \"json\"}\n",
    ")\n",
    "response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "# Parse JSON response\n",
    "results = response.json()\n",
    "\n",
    "# Print results - CORRECTED VARIABLES\n",
    "for binding in results[\"results\"][\"bindings\"]:\n",
    "    print(\n",
    "        \"Result:\", binding[\"result\"][\"value\"],\n",
    "        \"| Multiplicity:\", binding[\"multiplicity\"][\"value\"],\n",
    "        \"| Charge:\", binding[\"charge\"][\"value\"]\n",
    "    )\n",
    "\n",
    "# --- Query 4: Find DLPNO results with high T1 diagnostic values (> 0.02)\n",
    "sparql_query = \"\"\"\n",
    "PREFIX dl: <http://example.org/ontologies/DLPNO.owl#>\n",
    "\n",
    "SELECT ?result ?t1Diagnostic\n",
    "WHERE {\n",
    "  ?result a dl:DLPNOResult ;\n",
    "          dl:hasT1Diagnostic ?t1Diagnostic .\n",
    "  FILTER(?t1Diagnostic > 0.02)\n",
    "}\n",
    "ORDER BY DESC(?t1Diagnostic)\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "# Send HTTP request\n",
    "response = requests.get(\n",
    "    sparql_endpoint,\n",
    "    params={\"query\": sparql_query, \"format\": \"json\"}\n",
    ")\n",
    "response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "# Parse JSON response\n",
    "results = response.json()\n",
    "\n",
    "# Print results - CORRECTED to use actual selected variables\n",
    "for binding in results[\"results\"][\"bindings\"]:\n",
    "    print(\n",
    "        \"Result:\", binding[\"result\"][\"value\"],\n",
    "        \"| T1 Diagnostic:\", binding[\"t1Diagnostic\"][\"value\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Query 5: Find DLPNO results with source and coordinates   \n",
    "sparql_query = \"\"\"\n",
    "PREFIX dl: <http://example.org/ontologies/DLPNO.owl#>\n",
    "\n",
    "SELECT ?result ?source ?coordinates\n",
    "WHERE {\n",
    "  ?result a dl:DLPNOResult ;\n",
    "          dl:hasSource ?source ;\n",
    "          dl:hasInputCoordinates ?coordinates .\n",
    "}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "# Send HTTP request\n",
    "response = requests.get(\n",
    "    sparql_endpoint,\n",
    "    params={\"query\": sparql_query, \"format\": \"json\"}\n",
    ")\n",
    "response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "# Parse JSON response\n",
    "results = response.json()\n",
    "\n",
    "# Print results - Needs to be improved \n",
    "for binding in results[\"results\"][\"bindings\"]:\n",
    "    print(\n",
    "        \"Result:\", binding[\"result\"][\"value\"],\n",
    "        \"| Source:\", binding[\"source\"][\"value\"],\n",
    "        \"| Coordinates:\", binding[\"coordinates\"][\"value\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Query 6: Find DLPNO results with specific route sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are Jackson's output examples for the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here you can do all of your usual pandas manipulations\n",
    "df.iloc[0][[\"route_section\", \"input_coordinates\"]].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can reduce the amount of memory consumed by only loading the columns that you care about using columns=...\n",
    "df = pd.read_parquet(DLPNO_DATABASE_FPATH, columns=[\"source\", \"energy\"])\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and you filter out specific rows _when reading_ the database to further reduce memory consumption (and speed things up)\n",
    "# these statements can be complex, but the pandas docs explain it well:\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html\n",
    "df = pd.read_parquet(\n",
    "    DLPNO_DATABASE_FPATH,\n",
    "    # SKIP rows where...\n",
    "    filters=[\n",
    "        [\n",
    "            (  # multiplicity is equal to 1\n",
    "                \"multiplicity\",\n",
    "                \"=\",\n",
    "                1,\n",
    "            ),  # AND\n",
    "            (  # energy is less than -500\n",
    "                \"energy\",\n",
    "                \"<\",\n",
    "                -500,\n",
    "            ),\n",
    "        ],\n",
    "        [  # OR\n",
    "            (  # these two specific files\n",
    "                \"source\",\n",
    "                \"not in\",\n",
    "                (\n",
    "                    \"/data1/groups/co2_capture/reactant_product_calculation/ts_nho_round1/output/DLPNO_sp_f12/outputs/outputs_146/146857.log\",\n",
    "                    \"/data1/groups/co2_capture/reactant_product_calculation/ts_nho_round1/output/DLPNO_sp_f12/outputs/outputs_146/146989.log\",\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    ],\n",
    ")\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also filter out based on one row without actually loading it\n",
    "df = pd.read_parquet(DLPNO_DATABASE_FPATH, filters=[(\"run_time\", \"<\", 100)], columns=[\"source\", \"charge\"])\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can run everything from above using polars as well, and in my experience it uses less memory and is faster\n",
    "df = pl.read_parquet(DLPNO_DATABASE_FPATH)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a notable difference is that polars sets `memory_map=True` by default (pandas supports it, but is False and accessible via kwarg only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to pass filters to polars, you have to use the `pyarrow_options` argument (polars only supports limiting the number of rows in\n",
    "# in sequential order via `n_rows`)\n",
    "df = pl.read_parquet(\n",
    "    DLPNO_DATABASE_FPATH,\n",
    "    columns=[\"source\", \"charge\"],\n",
    "    pyarrow_options=dict(\n",
    "        filters=[(\"run_time\", \"<\", 100)],\n",
    "        schema=DLPNO_SCHEMA,\n",
    "    ),\n",
    ")\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...or just use polars other functions\n",
    "df = (\n",
    "    pl.scan_parquet(\n",
    "        DLPNO_DATABASE_FPATH,\n",
    "    )  # opens the file, but does not actually read it (LazyFrame)\n",
    "    .filter(\n",
    "        pl.col(\"run_time\") < 100,\n",
    "    )  # sets up our filters, but still does not run the query\n",
    "    .select(pl.col(\"source\"), pl.col(\"energy\"))\n",
    "    .collect()  # actually runs the query\n",
    ")\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final option is to interact with the data via pyarrow directly, which takes all the same arguments as before but in a slightly\n",
    "# different setup - this is the single fastest way to read the data\n",
    "table = pq.ParquetDataset(DLPNO_DATABASE_FPATH, schema=DLPNO_SCHEMA, filters=[(\"run_time\", \"<\", 100)]).read(columns=[\"source\", \"energy\"])\n",
    "df = table.to_pandas()\n",
    "df.head(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
