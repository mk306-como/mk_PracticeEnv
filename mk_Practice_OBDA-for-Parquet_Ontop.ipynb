{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupiter Notebook creates a parquet data base, creates and ontology and then constructs an OBDA mappint to SPARQL query the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Ontop as OBDA mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Required Libraries\n",
    "# %pip install pandas pyarrow rdflib SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read from Parquet file:\n",
      "      Name  Age         City\n",
      "0    Alice   25     New York\n",
      "1      Bob   30  Los Angeles\n",
      "2  Charlie   35      Chicago\n",
      "3    David   40      Houston\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create the Parquet File\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Define the schema for the Parquet file\n",
    "schema = pa.schema([\n",
    "    ('Name', pa.string()),\n",
    "    ('Age', pa.int32()),\n",
    "    ('City', pa.string())\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 35, 40],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a PyArrow Table and write to a Parquet file\n",
    "table = pa.Table.from_pandas(df, schema=schema)\n",
    "parquet_file = 'my_Parquet-file.parquet'\n",
    "pq.write_table(table, parquet_file)\n",
    "\n",
    "# Read the Parquet file back into a Pandas DataFrame\n",
    "table_read = pq.read_table(parquet_file)\n",
    "df_read = table_read.to_pandas()\n",
    "\n",
    "print(\"Data read from Parquet file:\")\n",
    "print(df_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N2119f5143a504a56a18f28474fc039cf (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Create the Ontology\n",
    "\n",
    "from rdflib import Graph, Namespace, Literal, URIRef\n",
    "\n",
    "# Create a new RDF Graph\n",
    "g = Graph()\n",
    "\n",
    "# Define a namespace for our ontology\n",
    "ns = Namespace(\"http://example.org/ontology/\")\n",
    "\n",
    "# Define classes and properties\n",
    "g.add((ns.Person, ns.type, ns.Class))\n",
    "g.add((ns.name, ns.type, ns.Property))\n",
    "g.add((ns.age, ns.type, ns.Property))\n",
    "g.add((ns.city, ns.type, ns.Property))\n",
    "\n",
    "# Serialize the ontology to a file\n",
    "g.serialize(destination='ontology.ttl', format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create the Ontop Mapping\n",
    "\n",
    "Now, we need to create an Ontop mapping file that maps the Parquet file's schema to the ontology. This is typically done using the Ontop CLI or a configuration file. For simplicity, we'll create a basic mapping file manually.\n",
    "\n",
    "Create a file named mapping.obda with the following content:\n",
    "[PrefixDeclaration]\n",
    ":       http://example.org/ontology/\n",
    "ex:     http://example.org/\n",
    "\n",
    "[MappingDeclaration] @collection [[\n",
    "mappingId   ~person_mapping\n",
    "target      :Person/{Name} a :Person ; :name {Name} ; :age {Age} ; :city {City} .\n",
    "source      SELECT Name, Age, City FROM my_Parquet-file.parquet\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARQL Query Results:\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Query the Parquet File Using SPARQL\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# Blazegraph endpoint can also write\n",
    "blazegraph_url = \"http://localhost:3838/blazegraph/namespace/kb/update\"\n",
    "\n",
    "\n",
    "# Define the SPARQL endpoint (assuming Ontop is running locally)\n",
    "sparql = SPARQLWrapper(blazegraph_url)\n",
    "\n",
    "# Define the SPARQL query\n",
    "query = \"\"\"\n",
    "PREFIX : <http://example.org/ontology/>\n",
    "SELECT ?name ?age ?city WHERE {\n",
    "  ?person a :Person ;\n",
    "          :name ?name ;\n",
    "          :age ?age ;\n",
    "          :city ?city .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Set the query and return format\n",
    "sparql.setQuery(query)\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "# Execute the query and print the results\n",
    "results = sparql.query().convert()\n",
    "\n",
    "print(\"SPARQL Query Results:\")\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
